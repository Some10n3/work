{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST data from CSV files\n",
    "train_data = pd.read_csv('mnist_train.csv')\n",
    "test_data = pd.read_csv('mnist_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate features and labels\n",
    "X_train, y_train = train_data.iloc[:, 1:].values, train_data.iloc[:, 0].values\n",
    "X_test, y_test = test_data.iloc[:, 1:].values, test_data.iloc[:, 0].values\n",
    "\n",
    "# Reduce dimensions using PCA (adjust n_components as needed)\n",
    "pca = PCA(n_components=50)  # 50 is arbitrary; adjust based on testing\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Define function to compute Mahalanobis distance for KNN\n",
    "def mahalanobis_distance(X_train, X_test):\n",
    "    cov_matrix = np.cov(X_train, rowvar=False)\n",
    "    inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "    distances = np.array([\n",
    "        [mahalanobis(x_test, x_train, inv_cov_matrix) for x_train in X_train]\n",
    "        for x_test in X_test\n",
    "    ])\n",
    "    return distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Mahalanobis distance\u001b[39;00m\n\u001b[0;32m     17\u001b[0m distances \u001b[38;5;241m=\u001b[39m mahalanobis_distance(X_train_pca, X_test_pca)\n\u001b[1;32m---> 18\u001b[0m y_pred_mahalanobis \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdistances\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_test_pca))])\n\u001b[0;32m     19\u001b[0m error_mahalanobis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m accuracy_score(y_test, y_pred_mahalanobis)\n\u001b[0;32m     20\u001b[0m errors_mahalanobis\u001b[38;5;241m.\u001b[39mappend(error_mahalanobis)\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "# Evaluate KNN with various values of k and both Euclidean and Mahalanobis distances\n",
    "k_values = range(1, 21)\n",
    "errors_euclidean = []\n",
    "errors_mahalanobis = []\n",
    "\n",
    "for k in k_values:\n",
    "    # Euclidean distance\n",
    "    knn_euclidean = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "    knn_euclidean.fit(X_train_pca, y_train)\n",
    "    y_pred_euclidean = knn_euclidean.predict(X_test_pca)\n",
    "    error_euclidean = 1 - accuracy_score(y_test, y_pred_euclidean)\n",
    "    errors_euclidean.append(error_euclidean)\n",
    "\n",
    "    # Mahalanobis distance\n",
    "    distances = mahalanobis_distance(X_train_pca, X_test_pca)\n",
    "    y_pred_mahalanobis = np.array([mode(y_train[distances[i].argsort()[:k]]).mode[0] for i in range(len(X_test_pca))])\n",
    "    error_mahalanobis = 1 - accuracy_score(y_test, y_pred_mahalanobis)\n",
    "    errors_mahalanobis.append(error_mahalanobis)\n",
    "\n",
    "\n",
    "# Plot error rates for different values of k\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(k_values, errors_euclidean, marker='o', label='Euclidean Distance')\n",
    "plt.plot(k_values, errors_mahalanobis, marker='s', label='Mahalanobis Distance')\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.title('Error Rate vs. Number of Neighbors (k) for Euclidean and Mahalanobis Distances')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number 2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST data from CSV files\n",
    "train_data = pd.read_csv('mnist_train.csv')\n",
    "test_data = pd.read_csv('mnist_test.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X_train, y_train = train_data.iloc[:, 1:].values, train_data.iloc[:, 0].values\n",
    "X_test, y_test = test_data.iloc[:, 1:].values, test_data.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions using PCA\n",
    "pca = PCA(n_components=50)  # Adjust n_components based on testing\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Define function to train GMM and calculate accuracy\n",
    "def gmm_accuracy(X_train, X_test, y_train, y_test, covariance_type):\n",
    "    gmm = GaussianMixture(n_components=10, covariance_type=covariance_type, random_state=0)\n",
    "    gmm.fit(X_train)\n",
    "    \n",
    "    y_train_pred = gmm.predict(X_train)\n",
    "    y_test_pred = gmm.predict(X_test)\n",
    "    \n",
    "    # Map predicted labels to actual labels using majority voting\n",
    "    label_map = {}\n",
    "    for label in np.unique(y_train):\n",
    "        mask = (y_train_pred == label)\n",
    "        label_map[label] = np.bincount(y_train[mask]).argmax()\n",
    "\n",
    "    y_test_mapped = np.vectorize(label_map.get)(y_test_pred)\n",
    "    accuracy = accuracy_score(y_test, y_test_mapped)\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate GMM with different covariance types\n",
    "covariance_types = ['full', 'tied', 'diag', 'spherical']\n",
    "accuracies = {}\n",
    "\n",
    "for cov_type in covariance_types:\n",
    "    accuracy = gmm_accuracy(X_train_pca, X_test_pca, y_train, y_test, cov_type)\n",
    "    accuracies[cov_type] = accuracy\n",
    "    print(f\"Covariance Type: {cov_type}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use hierarchical clustering to separate classes into layers\n",
    "def hierarchical_clustering(X, num_clusters=10):\n",
    "    Z = linkage(X, method='ward')\n",
    "    clusters = fcluster(Z, num_clusters, criterion='maxclust')\n",
    "    return clusters\n",
    "\n",
    "# Perform hierarchical clustering on the training data\n",
    "clusters = hierarchical_clustering(X_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GMM within each cluster\n",
    "cluster_accuracies = []\n",
    "for cluster_id in np.unique(clusters):\n",
    "    X_train_cluster = X_train_pca[clusters == cluster_id]\n",
    "    y_train_cluster = y_train[clusters == cluster_id]\n",
    "    \n",
    "    # Train GMM on this subset\n",
    "    gmm = GaussianMixture(n_components=10, covariance_type='full', random_state=0)\n",
    "    gmm.fit(X_train_cluster)\n",
    "    \n",
    "    y_test_pred_cluster = gmm.predict(X_test_pca)\n",
    "    accuracy_cluster = accuracy_score(y_test, y_test_pred_cluster)\n",
    "    cluster_accuracies.append(accuracy_cluster)\n",
    "\n",
    "print(f\"Average accuracy after hierarchical clustering: {np.mean(cluster_accuracies):.4f}\")\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(accuracies.keys(), accuracies.values())\n",
    "plt.xlabel('Covariance Type')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy of GMM with Different Covariance Types on MNIST')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number 3 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST data from CSV files\n",
    "train_data = pd.read_csv('mnist_train.csv')\n",
    "test_data = pd.read_csv('mnist_test.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X_train, y_train = train_data.iloc[:, 1:].values, train_data.iloc[:, 0].values\n",
    "X_test, y_test = test_data.iloc[:, 1:].values, test_data.iloc[:, 0].values\n",
    "\n",
    "# Initialize the Gaussian Naive Bayes classifier\n",
    "nb_classifier = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the training data\n",
    "nb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels on the training and testing data\n",
    "y_train_pred = nb_classifier.predict(X_train)\n",
    "y_test_pred = nb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display confusion matrices\n",
    "train_cm = confusion_matrix(y_train, y_train_pred)\n",
    "test_cm = confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ConfusionMatrixDisplay(train_cm).plot(ax=ax[0], cmap='Blues')\n",
    "ax[0].set_title(\"Training Confusion Matrix\")\n",
    "\n",
    "ConfusionMatrixDisplay(test_cm).plot(ax=ax[1], cmap='Blues')\n",
    "ax[1].set_title(\"Testing Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number 4 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST data from CSV files\n",
    "train_data = pd.read_csv('mnist_train.csv')\n",
    "test_data = pd.read_csv('mnist_test.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X_train, y_train = train_data.iloc[:, 1:].values, train_data.iloc[:, 0].values\n",
    "X_test, y_test = test_data.iloc[:, 1:].values, test_data.iloc[:, 0].values\n",
    "\n",
    "# Initialize and train a Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=0)\n",
    "dt_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for both training and testing data\n",
    "y_train_pred_dt = dt_classifier.predict(X_train)\n",
    "y_test_pred_dt = dt_classifier.predict(X_test)\n",
    "\n",
    "# Initialize and train a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=100,           # Number of trees in the forest\n",
    "    criterion='gini',           # Function to measure the quality of a split\n",
    "    max_depth=None,             # Maximum depth of the tree (None means nodes are expanded until pure)\n",
    "    min_samples_split=2,        # Minimum samples required to split a node\n",
    "    min_samples_leaf=1,         # Minimum samples required to be at a leaf node\n",
    "    max_features='sqrt',        # Number of features to consider when looking for the best split\n",
    "    bootstrap=True,             # Whether bootstrap samples are used when building trees\n",
    "    random_state=0\n",
    ")\n",
    "rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for both training and testing data\n",
    "y_train_pred_rf = rf_classifier.predict(X_train)\n",
    "y_test_pred_rf = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate and display confusion matrices for Decision Tree\n",
    "train_cm_dt = confusion_matrix(y_train, y_train_pred_dt)\n",
    "test_cm_dt = confusion_matrix(y_test, y_test_pred_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display confusion matrices for Random Forest\n",
    "train_cm_rf = confusion_matrix(y_train, y_train_pred_rf)\n",
    "test_cm_rf = confusion_matrix(y_test, y_test_pred_rf)\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, ax = plt.subplots(2, 2, figsize=(14, 12))\n",
    "ConfusionMatrixDisplay(train_cm_dt).plot(ax=ax[0, 0], cmap='Blues')\n",
    "ax[0, 0].set_title(\"Decision Tree - Training Confusion Matrix\")\n",
    "\n",
    "ConfusionMatrixDisplay(test_cm_dt).plot(ax=ax[0, 1], cmap='Blues')\n",
    "ax[0, 1].set_title(\"Decision Tree - Testing Confusion Matrix\")\n",
    "\n",
    "ConfusionMatrixDisplay(train_cm_rf).plot(ax=ax[1, 0], cmap='Greens')\n",
    "ax[1, 0].set_title(\"Random Forest - Training Confusion Matrix\")\n",
    "\n",
    "ConfusionMatrixDisplay(test_cm_rf).plot(ax=ax[1, 1], cmap='Greens')\n",
    "ax[1, 1].set_title(\"Random Forest - Testing Confusion Matrix\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number 5 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST data from CSV files\n",
    "train_data = pd.read_csv('mnist_train.csv')\n",
    "test_data = pd.read_csv('mnist_test.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X_train, y_train = train_data.iloc[:, 1:].values, train_data.iloc[:, 0].values\n",
    "X_test, y_test = test_data.iloc[:, 1:].values, test_data.iloc[:, 0].values\n",
    "\n",
    "# Initialize lists to store results for plotting\n",
    "c_values = [0.1, 1, 10, 100]\n",
    "train_errors_poly, test_errors_poly = [], []\n",
    "train_errors_rbf, test_errors_rbf = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial kernel SVM\n",
    "for C in c_values:\n",
    "    svm_poly = SVC(kernel='poly', degree=3, C=C, random_state=0)\n",
    "    svm_poly.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions and error calculation\n",
    "    y_train_pred_poly = svm_poly.predict(X_train)\n",
    "    y_test_pred_poly = svm_poly.predict(X_test)\n",
    "    train_errors_poly.append(np.mean(y_train != y_train_pred_poly))\n",
    "    test_errors_poly.append(np.mean(y_test != y_test_pred_poly))\n",
    "\n",
    "    # Display confusion matrix for C=1\n",
    "    if C == 1:\n",
    "        train_cm_poly = confusion_matrix(y_train, y_train_pred_poly)\n",
    "        test_cm_poly = confusion_matrix(y_test, y_test_pred_poly)\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        ConfusionMatrixDisplay(train_cm_poly).plot(ax=ax[0], cmap='Blues')\n",
    "        ax[0].set_title(\"SVM Polynomial Kernel - Training Confusion Matrix (C=1)\")\n",
    "        ConfusionMatrixDisplay(test_cm_poly).plot(ax=ax[1], cmap='Blues')\n",
    "        ax[1].set_title(\"SVM Polynomial Kernel - Testing Confusion Matrix (C=1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF kernel SVM\n",
    "for C in c_values:\n",
    "    svm_rbf = SVC(kernel='rbf', C=C, random_state=0)\n",
    "    svm_rbf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions and error calculation\n",
    "    y_train_pred_rbf = svm_rbf.predict(X_train)\n",
    "    y_test_pred_rbf = svm_rbf.predict(X_test)\n",
    "    train_errors_rbf.append(np.mean(y_train != y_train_pred_rbf))\n",
    "    test_errors_rbf.append(np.mean(y_test != y_test_pred_rbf))\n",
    "\n",
    "    # Display confusion matrix for C=1\n",
    "    if C == 1:\n",
    "        train_cm_rbf = confusion_matrix(y_train, y_train_pred_rbf)\n",
    "        test_cm_rbf = confusion_matrix(y_test, y_test_pred_rbf)\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        ConfusionMatrixDisplay(train_cm_rbf).plot(ax=ax[0], cmap='Greens')\n",
    "        ax[0].set_title(\"SVM RBF Kernel - Training Confusion Matrix (C=1)\")\n",
    "        ConfusionMatrixDisplay(test_cm_rbf).plot(ax=ax[1], cmap='Greens')\n",
    "        ax[1].set_title(\"SVM RBF Kernel - Testing Confusion Matrix (C=1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting loss for different values of C\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(c_values, train_errors_poly, label=\"Train Error (Poly Kernel)\", marker='o')\n",
    "plt.plot(c_values, test_errors_poly, label=\"Test Error (Poly Kernel)\", marker='o')\n",
    "plt.plot(c_values, train_errors_rbf, label=\"Train Error (RBF Kernel)\", marker='x')\n",
    "plt.plot(c_values, test_errors_rbf, label=\"Test Error (RBF Kernel)\", marker='x')\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"C (Soft Margin)\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate vs. C for Polynomial and RBF Kernels\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
